import os
import torch
import torch.nn as nn
import torch.optim as optim
import albumentations as A
import datetime
import logging
from torch.utils.data import DataLoader
from transformers import OwlViTProcessor, OwlViTForObjectDetection
from peft import LoraConfig, get_peft_model
from dataset import ImageTextBBoxDataset, collate_fn  # ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ ëª¨ë“ˆ
from loss import HungarianMatcher, OWLVITLoss              # ì‚¬ìš©ì ì •ì˜ ì†ì‹¤í•¨ìˆ˜

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

class OWLVITCLIPModel:
    """
    OwlViT ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , LoRAë¥¼ ì ìš©í•œ í›„ headë§Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    ì—¬ê¸°ì„œëŠ” bbox ì˜ˆì¸¡ head(box_head)ì™€ í´ë˜ìŠ¤ ì˜ˆì¸¡ head(class_head)ë§Œ í•™ìŠµí•©ë‹ˆë‹¤.
    """
    def __init__(self, model_name="google/owlvit-base-patch32", device='cuda', use_lora=True, lora_config_params=None):
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        # í”„ë¡œì„¸ì„œ ë° ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        self.processor = OwlViTProcessor.from_pretrained(model_name)
        self.model = OwlViTForObjectDetection.from_pretrained(model_name).to(self.device)
        self.model.train()

        # ì „ì²´ íŒŒë¼ë¯¸í„° Freeze
        for param in self.model.parameters():
            param.requires_grad = False

        if use_lora:
            # ê¸°ë³¸ LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ (í•„ìš”ì‹œ ì¡°ì •)
            if lora_config_params is None:
                lora_config_params = {"r": 4, "lora_alpha": 32, "lora_dropout": 0.1}
            lora_config = LoraConfig(
                task_type="OTHER",  # íƒœìŠ¤í¬ì— ë”°ë¼ ì ì ˆí•œ task_typeìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥
                r=lora_config_params["r"],
                lora_alpha=lora_config_params["lora_alpha"],
                lora_dropout=lora_config_params["lora_dropout"],
                target_modules=["text_projection", "visual_projection"]
            )
            # PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ LoRA ì–´ëŒ‘í„° ì¶”ê°€
            self.model = get_peft_model(self.model, lora_config)
        else:
            # LoRAë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°, ì˜ˆì‹œë¡œ text_projection, visual_projectionë§Œ unfreeze
            trainable_layers = [
                self.model.owlvit.text_projection,
                self.model.owlvit.visual_projection
            ]
            for layer in trainable_layers:
                for param in layer.parameters():
                    param.requires_grad = True
            self.model.owlvit.logit_scale.requires_grad = True

        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        logging.info(f"ğŸš€ ì´ˆê¸° trainable íŒŒë¼ë¯¸í„°: {trainable_params / 1e6:.2f}M")

    def load_checkpoint(self, checkpoint_path):
        """
        checkpointì—ì„œ ëª¨ë¸ state_dictë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint["model_state_dict"])
        logging.info(f"Checkpoint loaded from {checkpoint_path}")

    def freeze_except_heads(self):
        """
        ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ freezeí•˜ê³ , 'box_head'ì™€ 'class_head'ì— í•´ë‹¹í•˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        for name, param in self.model.named_parameters():
            if "box_head" in name or "class_head" in name:
                param.requires_grad = True
            else:
                param.requires_grad = False
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        logging.info(f"ğŸš€ Headë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •ë¨. Trainable íŒŒë¼ë¯¸í„°: {trainable_params / 1e6:.2f}M")

    def reinitialize_heads(self):
        """
        box_headì™€ class_headì— í•´ë‹¹í•˜ëŠ” ëª¨ë“ˆë“¤ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¬ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        def _reinit_module(module, module_name):
            if hasattr(module, "reset_parameters"):
                module.reset_parameters()
                logging.info(f"{module_name} ì¬ì´ˆê¸°í™”ë¨.")
        for name, module in self.model.named_modules():
            if "box_head" in name or "class_head" in name:
                _reinit_module(module, name)

    def get_optimizer(self, lr=1e-4):
        """í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°(ì—¬ê¸°ì„œëŠ” headë§Œ)ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì˜µí‹°ë§ˆì´ì € ë°˜í™˜"""
        return optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr)

    def get_dataloaders(self, train_dir, val_dir, batch_size=16):
        """ë°ì´í„° ë¡œë” ìƒì„±"""
        transform = A.Compose([
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.RandomBrightnessContrast(p=0.5),
            A.GaussianBlur(p=0.3),
            A.GaussNoise(p=0.3),
            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=30, p=0.5),
        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))


        train_dataset = ImageTextBBoxDataset(train_dir, self.processor, transform=transform)
        val_dataset = ImageTextBBoxDataset(val_dir, self.processor)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
        return train_loader, val_loader

    def train(self, train_dir, val_dir, epochs=10, batch_size=16, lr=1e-4, ckpt_base_dir="ckpt"):
        """
        í•™ìŠµ ë° ê²€ì¦ ë£¨í”„.
        í•™ìŠµ ì „ì— freeze_except_heads()ë¥¼ í˜¸ì¶œí•˜ì—¬ headë§Œ í•™ìŠµí•˜ë„ë¡ í•©ë‹ˆë‹¤.
        """
        # headë§Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
        self.freeze_except_heads()

        train_loader, val_loader = self.get_dataloaders(train_dir, val_dir, batch_size)
        optimizer = self.get_optimizer(lr)
        matcher = HungarianMatcher(cost_class=1, cost_bbox=5, cost_giou=2)
        weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}
        criterion = OWLVITLoss(num_classes=2, matcher=matcher, weight_dict=weight_dict, eos_coef=0.1, losses=['labels', 'boxes'])
    

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í´ë” ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        ckpt_dir = os.path.join(ckpt_base_dir, timestamp)
        os.makedirs(ckpt_dir, exist_ok=True)

        best_val_loss = float("inf")

        for epoch in range(epochs):
            self.model.train()
            total_loss = 0.0
            for batch in train_loader:
                optimizer.zero_grad()
                pixel_values = batch["pixel_values"].to(self.device)
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                bboxes = batch['bboxes']
                all_labels = []  # ì´ë¯¸ì§€ë³„ë¡œ label ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥ (ê° ì´ë¯¸ì§€: tensor of shape (num_boxes,))
                for bbox_tensor in bboxes:
                    image_labels = []
                    for bbox in bbox_tensor:  # bboxëŠ” (4,) í…ì„œ
                        # bboxì˜ í•©ì´ 0ì´ë©´ label 0, ì•„ë‹ˆë©´ 1ë¡œ ì§€ì •
                        label = 0 if bbox.sum().item() == 0 else 1
                        image_labels.append(label)
                    all_labels.append(torch.tensor(image_labels, dtype=torch.int64))
                all_labels = [torch.tensor(image_labels, dtype=torch.int64, device=self.device) for image_labels in all_labels]
                bboxes = [bbox.to(self.device) for bbox in bboxes]

                # import ipdb; ipdb.set_trace()
                
                
                outputs = self.model(
                    pixel_values=pixel_values,
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                outputs = {
                    "pred_logits": outputs.logits,
                    "pred_boxes": outputs.pred_boxes
                }
                targets = [{"labels": lbl, "boxes": box} for lbl, box in zip(all_labels, bboxes)]

                loss = criterion(outputs, targets)
                # 'loss_ce': weight, 'loss_bbox': weight, 'loss_giou': weight
                loss = loss['total_loss']
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
                break
            avg_train_loss = total_loss / len(train_loader)
            avg_val_loss = self.validate(val_loader, criterion)
            logging.info(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            checkpoint = {
                "epoch": epoch + 1,
                "model_state_dict": self.model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "train_loss": avg_train_loss,
                "val_loss": avg_val_loss
            }
            ckpt_path = os.path.join(ckpt_dir, f"epoch_{epoch+1}.pth")
            torch.save(checkpoint, ckpt_path)
            logging.info(f"Checkpoint saved: {ckpt_path}")

            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_ckpt_path = os.path.join(ckpt_dir, "best_model.pth")
                torch.save(checkpoint, best_ckpt_path)
                logging.info(f"Best model updated: {best_ckpt_path}")

    def validate(self, val_loader, criterion):
        """ê²€ì¦ ë£¨í”„ - trainê³¼ ë™ì¼í•œ íƒ€ê²Ÿ êµ¬ì„± ë°©ì‹ì„ ì‚¬ìš©"""
        self.model.eval()
        total_loss = 0.0
        with torch.no_grad():
            for batch in val_loader:
                pixel_values = batch["pixel_values"].to(self.device)
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                bboxes = batch['bboxes']
                
                # ê° ì´ë¯¸ì§€ì— ëŒ€í•œ ë¼ë²¨ ìƒì„± (bbox í•©ì´ 0ì´ë©´ 0, ì•„ë‹ˆë©´ 1)
                all_labels = []
                for bbox_tensor in bboxes:
                    image_labels = []
                    for bbox in bbox_tensor:  # bboxëŠ” (4,) í…ì„œ
                        label = 0 if bbox.sum().item() == 0 else 1
                        image_labels.append(label)
                    all_labels.append(torch.tensor(image_labels, dtype=torch.int64, device=self.device))
                
                # bboxesë„ device ì´ë™ (ë§Œì•½ ì´ë¯¸ tensorë¼ë©´)
                bboxes = [bbox.to(self.device) for bbox in bboxes]
                
                # ëª¨ë¸ ì¶”ë¡  ë° ê²°ê³¼ êµ¬ì„±
                outputs = self.model(
                    pixel_values=pixel_values,
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                outputs = {
                    "pred_logits": outputs.logits,
                    "pred_boxes": outputs.pred_boxes
                }
                
                # ê° ì´ë¯¸ì§€ì— ëŒ€í•œ íƒ€ê²Ÿ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                targets = [{"labels": lbl, "boxes": box} for lbl, box in zip(all_labels, bboxes)]
                
                loss = criterion(outputs, targets)
                loss = loss['total_loss']
                total_loss += loss.item()
        return total_loss / len(val_loader)


if __name__ == "__main__":
    # ë°ì´í„°ì…‹ ê²½ë¡œ (í”„ë¡œì íŠ¸ì— ë§ê²Œ ìˆ˜ì •)
    train_dataset_dir = "./total_dataset/train_dataset/"
    val_dataset_dir = "./total_dataset/val/"

    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (LoRA ì ìš©)
    model_wrapper = OWLVITCLIPModel(use_lora=True)

    # ê¸°ì¡´ checkpointì—ì„œ ëª¨ë¸ ë¡œë“œ (ì›í•œë‹¤ë©´ head ì¬ì´ˆê¸°í™”ë„ ìˆ˜í–‰)
    # checkpoint_path = "./ckpt/20250313_172710/best_model.pth"
    checkpoint_path = './ckpt/20250313_184959/best_model.pth'
    model_wrapper.load_checkpoint(checkpoint_path)
    # (ì›í•˜ëŠ” ê²½ìš°) head ì¬ì´ˆê¸°í™”
    model_wrapper.reinitialize_heads()

    # headë§Œ í•™ìŠµí•˜ë„ë¡ ì„¤ì •í•œ í›„ í•™ìŠµ ì‹œì‘
    model_wrapper.train(
        train_dir=train_dataset_dir,
        val_dir=val_dataset_dir,
        epochs=10,
        batch_size=16,
        lr=1e-4,
        ckpt_base_dir="ckpt"
    )
