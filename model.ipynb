{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"./ckpt/20250313_172710/best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from clip_dataset import ImageTextDataset, collate_fn  # ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ ëª¨ë“ˆ\n",
    "from loss import CLIPContrastiveLoss              # ì‚¬ìš©ì ì •ì˜ ì†ì‹¤í•¨ìˆ˜\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "class OWLVITCLIPModel:\n",
    "    \"\"\"\n",
    "    OwlViT ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , LoRAë¥¼ ì ìš©í•œ í›„ headë§Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
    "    ì—¬ê¸°ì„œëŠ” bbox ì˜ˆì¸¡ head(box_head)ì™€ í´ë˜ìŠ¤ ì˜ˆì¸¡ head(class_head)ë§Œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"google/owlvit-base-patch32\", device='cuda', use_lora=True, lora_config_params=None):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "        # í”„ë¡œì„¸ì„œ ë° ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ\n",
    "        self.processor = OwlViTProcessor.from_pretrained(model_name)\n",
    "        self.model = OwlViTForObjectDetection.from_pretrained(model_name).to(self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        # ì „ì²´ íŒŒë¼ë¯¸í„° Freeze\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if use_lora:\n",
    "            # ê¸°ë³¸ LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ (í•„ìš”ì‹œ ì¡°ì •)\n",
    "            if lora_config_params is None:\n",
    "                lora_config_params = {\"r\": 4, \"lora_alpha\": 32, \"lora_dropout\": 0.1}\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=\"OTHER\",  # íƒœìŠ¤í¬ì— ë”°ë¼ ì ì ˆí•œ task_typeìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥\n",
    "                r=lora_config_params[\"r\"],\n",
    "                lora_alpha=lora_config_params[\"lora_alpha\"],\n",
    "                lora_dropout=lora_config_params[\"lora_dropout\"],\n",
    "                target_modules=[\"text_projection\", \"visual_projection\"]\n",
    "            )\n",
    "            # PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ LoRA ì–´ëŒ‘í„° ì¶”ê°€\n",
    "            self.model = get_peft_model(self.model, lora_config)\n",
    "        else:\n",
    "            # LoRAë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°, ì˜ˆì‹œë¡œ text_projection, visual_projectionë§Œ unfreeze\n",
    "            trainable_layers = [\n",
    "                self.model.owlvit.text_projection,\n",
    "                self.model.owlvit.visual_projection\n",
    "            ]\n",
    "            for layer in trainable_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "            self.model.owlvit.logit_scale.requires_grad = True\n",
    "\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        logging.info(f\"ğŸš€ ì´ˆê¸° trainable íŒŒë¼ë¯¸í„°: {trainable_params / 1e6:.2f}M\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"\n",
    "        checkpointì—ì„œ ëª¨ë¸ state_dictë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        logging.info(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "\n",
    "    def freeze_except_heads(self):\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ freezeí•˜ê³ , 'box_head'ì™€ 'class_head'ì— í•´ë‹¹í•˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"box_head\" in name or \"class_head\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        logging.info(f\"ğŸš€ Headë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •ë¨. Trainable íŒŒë¼ë¯¸í„°: {trainable_params / 1e6:.2f}M\")\n",
    "\n",
    "    def reinitialize_heads(self):\n",
    "        \"\"\"\n",
    "        box_headì™€ class_headì— í•´ë‹¹í•˜ëŠ” ëª¨ë“ˆë“¤ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¬ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        def _reinit_module(module, module_name):\n",
    "            if hasattr(module, \"reset_parameters\"):\n",
    "                module.reset_parameters()\n",
    "                logging.info(f\"{module_name} ì¬ì´ˆê¸°í™”ë¨.\")\n",
    "        for name, module in self.model.named_modules():\n",
    "            if \"box_head\" in name or \"class_head\" in name:\n",
    "                _reinit_module(module, name)\n",
    "\n",
    "    def get_optimizer(self, lr=1e-4):\n",
    "        \"\"\"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°(ì—¬ê¸°ì„œëŠ” headë§Œ)ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì˜µí‹°ë§ˆì´ì € ë°˜í™˜\"\"\"\n",
    "        return optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr)\n",
    "\n",
    "    def get_dataloaders(self, train_dir, val_dir, batch_size=16):\n",
    "        \"\"\"ë°ì´í„° ë¡œë” ìƒì„±\"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        ])\n",
    "\n",
    "        train_dataset = ImageTextDataset(train_dir, self.processor, transform=transform)\n",
    "        val_dataset = ImageTextDataset(val_dir, self.processor)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train(self, train_dir, val_dir, epochs=10, batch_size=16, lr=1e-4, ckpt_base_dir=\"ckpt\"):\n",
    "        \"\"\"\n",
    "        í•™ìŠµ ë° ê²€ì¦ ë£¨í”„.\n",
    "        í•™ìŠµ ì „ì— freeze_except_heads()ë¥¼ í˜¸ì¶œí•˜ì—¬ headë§Œ í•™ìŠµí•˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        # headë§Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •\n",
    "        self.freeze_except_heads()\n",
    "\n",
    "        train_loader, val_loader = self.get_dataloaders(train_dir, val_dir, batch_size)\n",
    "        optimizer = self.get_optimizer(lr)\n",
    "        criterion = CLIPContrastiveLoss().to(self.device)\n",
    "\n",
    "        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í´ë” ìƒì„±\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        ckpt_dir = os.path.join(ckpt_base_dir, timestamp)\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                # bbox ë° class head í•™ìŠµì€ ë³´í†µ image_embeds, text_embedsê°€ ì•„ë‹Œ\n",
    "                # box_headì™€ class_headì˜ ì¶œë ¥ì„ ì´ìš©í•©ë‹ˆë‹¤.\n",
    "                # (ì•„ë˜ ì˜ˆì‹œëŠ” ë‹¨ìˆœíˆ ê¸°ì¡´ ì„ë² ë”© ëŒ€ë¹„ ì†ì‹¤ì„ ê³„ì‚°í•˜ëŠ” ì˜ˆì‹œì´ë©°,\n",
    "                # ì‹¤ì œ bbox ë° class head í•™ìŠµì—ëŠ” ì ì ˆí•œ ì†ì‹¤ í•¨ìˆ˜ ë° ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.)\n",
    "                vision_embeds = outputs.image_embeds.mean(dim=(1, 2))\n",
    "                text_embeds = outputs.text_embeds.squeeze(1)\n",
    "                vision_embeds = self.model.owlvit.visual_projection(vision_embeds)\n",
    "                text_embeds = self.model.owlvit.text_projection(text_embeds)\n",
    "\n",
    "                loss = criterion(vision_embeds, text_embeds)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            avg_val_loss = self.validate(val_loader, criterion)\n",
    "            logging.info(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss\n",
    "            }\n",
    "            ckpt_path = os.path.join(ckpt_dir, f\"epoch_{epoch+1}.pth\")\n",
    "            torch.save(checkpoint, ckpt_path)\n",
    "            logging.info(f\"Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_ckpt_path = os.path.join(ckpt_dir, \"best_model.pth\")\n",
    "                torch.save(checkpoint, best_ckpt_path)\n",
    "                logging.info(f\"Best model updated: {best_ckpt_path}\")\n",
    "\n",
    "    def validate(self, val_loader, criterion):\n",
    "        \"\"\"ê²€ì¦ ë£¨í”„\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                vision_embeds = outputs.image_embeds.mean(dim=(1, 2))\n",
    "                text_embeds = outputs.text_embeds.squeeze(1)\n",
    "                vision_embeds = self.model.owlvit.visual_projection(vision_embeds)\n",
    "                text_embeds = self.model.owlvit.text_projection(text_embeds)\n",
    "\n",
    "                loss = criterion(vision_embeds, text_embeds)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ë°ì´í„°ì…‹ ê²½ë¡œ (í”„ë¡œì íŠ¸ì— ë§ê²Œ ìˆ˜ì •)\n",
    "    train_dataset_dir = \"./total_dataset/train_dataset/\"\n",
    "    val_dataset_dir = \"./total_dataset/val/\"\n",
    "\n",
    "    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (LoRA ì ìš©)\n",
    "    model_wrapper = OWLVITCLIPModel(use_lora=True)\n",
    "\n",
    "    # ê¸°ì¡´ checkpointì—ì„œ ëª¨ë¸ ë¡œë“œ (ì›í•œë‹¤ë©´ head ì¬ì´ˆê¸°í™”ë„ ìˆ˜í–‰)\n",
    "    checkpoint_path = \"./ckpt/20250313_172710/best_model.pth\"\n",
    "    model_wrapper.load_checkpoint(checkpoint_path)\n",
    "    # (ì›í•˜ëŠ” ê²½ìš°) head ì¬ì´ˆê¸°í™”\n",
    "    # model_wrapper.reinitialize_heads()\n",
    "\n",
    "    # headë§Œ í•™ìŠµí•˜ë„ë¡ ì„¤ì •í•œ í›„ í•™ìŠµ ì‹œì‘\n",
    "    model_wrapper.train(\n",
    "        train_dir=train_dataset_dir,\n",
    "        val_dir=val_dataset_dir,\n",
    "        epochs=10,\n",
    "        batch_size=16,\n",
    "        lr=1e-4,\n",
    "        ckpt_base_dir=\"ckpt\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 18:37:16,478 - INFO - ğŸš€ Trainable Parameters: 0.01M\n"
     ]
    }
   ],
   "source": [
    "model = OWLVITCLIPModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OwlViTForObjectDetection(\n",
       "      (owlvit): OwlViTModel(\n",
       "        (text_model): OwlViTTextTransformer(\n",
       "          (embeddings): OwlViTTextEmbeddings(\n",
       "            (token_embedding): Embedding(49408, 512)\n",
       "            (position_embedding): Embedding(16, 512)\n",
       "          )\n",
       "          (encoder): OwlViTEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-11): 12 x OwlViTEncoderLayer(\n",
       "                (self_attn): OwlViTAttention(\n",
       "                  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): OwlViTMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (vision_model): OwlViTVisionTransformer(\n",
       "          (embeddings): OwlViTVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "            (position_embedding): Embedding(577, 768)\n",
       "          )\n",
       "          (pre_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): OwlViTEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-11): 12 x OwlViTEncoderLayer(\n",
       "                (self_attn): OwlViTAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): OwlViTMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (visual_projection): lora.Linear(\n",
       "          (base_layer): Linear(in_features=768, out_features=512, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (text_projection): lora.Linear(\n",
       "          (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=512, out_features=4, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=4, out_features=512, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (class_head): OwlViTClassPredictionHead(\n",
       "        (dense0): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (logit_shift): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (logit_scale): Linear(in_features=768, out_features=1, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "      )\n",
       "      (box_head): OwlViTBoxPredictionHead(\n",
       "        (dense0): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dense1): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (dense2): Linear(in_features=768, out_features=4, bias=True)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.20 ('py38_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0e5413880e9d5a5e633f786f0b1cbaf87005dea0ec54f3e8eb92b5684f13c71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
