import torch
import torch.nn as nn
import torch.optim as optim
import logging
import os
import datetime
from torch.utils.data import DataLoader
from torchvision import transforms
from transformers import OwlViTProcessor, OwlViTForObjectDetection
from clip_dataset import ImageTextDataset, collate_fn
from loss import CLIPContrastiveLoss
from peft import LoraConfig, get_peft_model

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def create_ckpt_dir():
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í´ë” ìƒì„±"""
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    ckpt_dir = f"ckpt/{timestamp}"
    os.makedirs(ckpt_dir, exist_ok=True)
    return ckpt_dir

def get_dataloaders(processor, train_dir, val_dir, batch_size=5):
    """ë°ì´í„° ë¡œë” ìƒì„±"""
    transform = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),  # ì¢Œìš° ë°˜ì „
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # ìƒ‰ìƒ ë³€í™”
        transforms.RandomRotation(degrees=15),  # íšŒì „
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # ì´ë™ ë³€í™˜
    ])

    train_dataset = ImageTextDataset(train_dir, processor, transform=transform)
    val_dataset = ImageTextDataset(val_dir, processor)

    train_dataloader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn
    )
    val_dataloader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn
    )
    return train_dataloader, val_dataloader

class OWLVITCLIPModel:
    """
    OwlViT ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , LoRAë¥¼ ì ìš©í•œ í›„ í•™ìŠµ/ê²€ì¦ ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê¸°ëŠ¥ì„ í¬í•¨í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    def __init__(self, model_name="google/owlvit-base-patch32", use_lora=True, lora_config_params=None):
        # í”„ë¡œì„¸ì„œ ë° ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        self.processor = OwlViTProcessor.from_pretrained(model_name)
        self.model = OwlViTForObjectDetection.from_pretrained(model_name).to(device)
        self.model.train()

        # ì „ì²´ íŒŒë¼ë¯¸í„° Freeze
        for param in self.model.parameters():
            param.requires_grad = False

        if use_lora:
            # ê¸°ë³¸ LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ (í•„ìš”ì‹œ ì¡°ì •)
            if lora_config_params is None:
                lora_config_params = {"r": 4, "lora_alpha": 32, "lora_dropout": 0.1}
            lora_config = LoraConfig(
                task_type="OTHER",  # íƒœìŠ¤í¬ì— ë”°ë¼ ì ì ˆí•œ task_typeìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥
                r=lora_config_params["r"],
                lora_alpha=lora_config_params["lora_alpha"],
                lora_dropout=lora_config_params["lora_dropout"],
                target_modules=["text_projection", "visual_projection"]
            )
            # PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ LoRA ì–´ëŒ‘í„° ì¶”ê°€
            self.model = get_peft_model(self.model, lora_config)
        else:
            # LoRAë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°, íŠ¹ì • ë ˆì´ì–´ë§Œ Unfreeze
            trainable_layers = [
                self.model.owlvit.text_projection,
                self.model.owlvit.visual_projection
            ]
            for layer in trainable_layers:
                for param in layer.parameters():
                    param.requires_grad = True
            self.model.owlvit.logit_scale.requires_grad = True

        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        logging.info(f"ğŸš€ Trainable Parameters: {trainable_params / 1e6:.2f}M")

    def get_optimizer(self, lr=1e-4):
        """ì˜µí‹°ë§ˆì´ì € ë°˜í™˜ (LoRA ì–´ëŒ‘í„° íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸)"""
        return optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr)

    def train(self, train_dir, val_dir, epochs=100, batch_size=16, lr=1e-4):
        """ëª¨ë¸ í•™ìŠµ"""
        train_dataloader, val_dataloader = get_dataloaders(self.processor, train_dir, val_dir, batch_size)
        optimizer = self.get_optimizer(lr)
        contrastive_loss = CLIPContrastiveLoss().to(device)
        ckpt_dir = create_ckpt_dir()
        best_val_loss = float("inf")

        for epoch in range(epochs):
            total_loss = 0
            self.model.train()

            for batch in train_dataloader:
                optimizer.zero_grad()
                pixel_values = batch["pixel_values"].to(device)
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)

                outputs = self.model(
                    pixel_values=pixel_values,
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                # ë¹„ì „ ë° í…ìŠ¤íŠ¸ ì„ë² ë”© ì²˜ë¦¬
                vision_embeds = outputs.image_embeds.mean(dim=(1, 2))
                text_embeds = outputs.text_embeds.squeeze(1)

                # í”„ë¡œì ì…˜ ë ˆì´ì–´ ì ìš©
                vision_embeds = self.model.owlvit.visual_projection(vision_embeds)
                text_embeds = self.model.owlvit.text_projection(text_embeds)

                loss = contrastive_loss(vision_embeds, text_embeds)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
                break
            val_loss = self.validate(val_dataloader, contrastive_loss)
            logging.info(f"Epoch {epoch+1} | Train Loss: {total_loss / len(train_dataloader):.4f} | Val Loss: {val_loss:.4f}")

            best_val_loss = self.save_checkpoint(optimizer, epoch, total_loss, val_loss, ckpt_dir, best_val_loss)

    def validate(self, dataloader, contrastive_loss):
        """ê²€ì¦ ë£¨í”„"""
        self.model.eval()
        total_loss = 0
        with torch.no_grad():
            for batch in dataloader:
                pixel_values = batch["pixel_values"].to(device)
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)

                outputs = self.model(
                    pixel_values=pixel_values,
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                vision_embeds = outputs.image_embeds.mean(dim=(1, 2))
                text_embeds = outputs.text_embeds.squeeze(1)

                vision_embeds = self.model.owlvit.visual_projection(vision_embeds)
                text_embeds = self.model.owlvit.text_projection(text_embeds)

                loss = contrastive_loss(vision_embeds, text_embeds)
                total_loss += loss.item()
        return total_loss / len(dataloader)

    def save_checkpoint(self, optimizer, epoch, train_loss, val_loss, ckpt_dir, best_val_loss):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch + 1,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss
        }
        torch.save(checkpoint, f"{ckpt_dir}/epoch_{epoch+1}.pth")
        if val_loss < best_val_loss:
            torch.save(checkpoint, f"{ckpt_dir}/best_model.pth")
            logging.info(f"ğŸ”¹ Best model updated at {ckpt_dir}/best_model.pth")
            best_val_loss = val_loss
        return best_val_loss